LDA整体流程
先定义一些字母的含义：
文档集合D，topic集合T
D中每个文档d看作一个单词序列< w1,w2,…,wn >，wi表示第i个单词，设d有n个单词。（LDA里面称之为 word bag ，实际上每个单词的出现位置对LDA算法无影响）
D中涉及的所有不同单词组成一个大集合VOCABULARY（简称VOC）
LDA以文档集合D作为输入（会有切词，去停用词，取词干等常见的预处理，略去不表），希望训练出的两个结果向量（设聚成k个Topic，VOC中共包含m个词）：

对每个D中的文档d，对应到不同topic的概率 θ d < p t1 ,…, p tk > ，其中，p ti 表示d对应T中第i个topic的概率。计算方法是直观的，p ti =n ti /n，其中n ti 表示d中对应第i个topic的词的数目，n是d中所有词的总数。
对每个T中的topic t，生成不同单词的概率 φ t < p w1 ,…, p wm > ，其中，p wi 表示t生成VOC中第i个单词的概率。计算方法同样很直观，p wi =N wi /N，其中N wi 表示对应到topic t的VOC中第i个单词的数目，N表示所有对应到topic t的单词总数。
LDA的核心公式如下：

p(w|d) = p(w|t)*p(t|d)

直观的看这个公式，就是以Topic作为中间层，可以通过当前的θ d 和φ t 给出了文档d中出现单词w的概率。其中p(t|d)利用θ d 计算得到，p(w|t)利用φ t 计算得到。 实际上，利用当前的θ d 和φ t ，我们可以为一个文档中的一个单词计算它对应任意一个Topic时的p(w|d)，然后根据这些结果来更新这个词应该对应的topic。然后，如果这个更新改变了这个单词所对应的Topic，就会反过来影响θ d 和φ t 。

LDA学习过程 LDA算法开始时，先随机地给θ d 和φ t 赋值（对所有的d和t）。然后上述过程不断重复，最终收敛到的结果就是LDA的输出。再详细说一下这个迭代的学习过程： 1）针对一个特定的文档d s 中的第i单词w i ，如果令该单词对应的topic为t j ，可以把上述公式改写为： p j (w i |d s ) = p(w i |t j )*p(t j |d s ) 先不管这个值怎么计算（可以先理解成直接从θ ds 和φ tj 中取对应的项。实际没这么简单，但对理解整个LDA流程没什么影响，后文再说）。 2）现在我们可以枚举T中的topic，得到所有的p j (w i |d s )，其中j取值1~k。然后可以根据这些概率值结果为d s 中的第i个单词w i 选择一个topic。最简单的想法是取令p j (w i |d s )最大的t j （注意，这个式子里只有j是变量），即 argmax[j]p j (w i |d s ) 当然这只是一种方法（好像还不怎么常用），实际上这里怎么选择t在学术界有很多方法，我还没有好好去研究。 3）然后，如果d s 中的第i个单词w i 在这里选择了一个与原先不同的topic，就会对θ d 和φ t 有影响了（根据前面提到过的这两个向量的计算公式可以很容易知道）。它们的影响又会反过来影响对上面提到的p(w|d)的计算。对D中所有的d中的所有w进行一次p(w|d)的计算并重新选择topic看作一次迭代。这样进行n次循环迭代之后，就会收敛到LDA所需要的结果了。